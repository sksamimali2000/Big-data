{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8ShGJU/x03LN9IJQKvoag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sksamimali2000/Big-data/blob/main/Hadoop_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Z0YZR4CJhc",
        "outputId": "e67e1f91-292e-4b5e-9da1-d71e7c4dc355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Hands-On-Hadoop-on-Google-Colab'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 11 (delta 2), reused 11 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (11/11), 16.25 KiB | 2.71 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ahfay/Hands-On-Hadoop-on-Google-Colab.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Hands-On-Hadoop-on-Google-Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-WE82PRDprA",
        "outputId": "7cafd677-4718-46a7-90cf-0c3eb8e623de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Hands-On-Hadoop-on-Google-Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Hadoop_Hands_On.ipynb"
      ],
      "metadata": {
        "id": "ToXdRCjvEnDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the installed Java version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9waDhkWoGkaG",
        "outputId": "e4598626-8e1e-4834-ed7c-7d87e0ff38f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.26\" 2025-01-21\n",
            "OpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# -q, quiet level 2: no output except for errors\n",
        "#> /dev/null on the end of any command where you want to redirect all the stdout into nothingness"
      ],
      "metadata": {
        "id": "LknlGvzeGssY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching java version to use as default (choose option 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SvRPuxlGt2g",
        "outputId": "ac6065fa-738b-4b98-995c-7b68a62cba3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Java default version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqFMRfXWHf3m",
        "outputId": "d74f1116-bdc0-435f-ca06-b7640777f408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_442\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_442-8u442-b06~us1-0ubuntu1~22.04-b06)\n",
            "OpenJDK 64-Bit Server VM (build 25.442-b06, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP3IXpI0HhX5",
        "outputId": "758bd5c4-bb11-43c0-cca7-9496b8e0a8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing os module\n",
        "import os\n",
        "#Creating environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "rbuVBxofH2sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It is good practice to purge before installation\n",
        "!apt-get purge openssh-server -qq"
      ],
      "metadata": {
        "id": "kAxteowYIK7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing openssh-server\n",
        "!apt-get install openssh-server -qq > /dev/null"
      ],
      "metadata": {
        "id": "UXPDCIzeILt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADh5kEUmIgOG",
        "outputId": "0955f8a2-ae9b-40f0-a219-380645f8383d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep Port /etc/ssh/sshd_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv7e6WpOIofd",
        "outputId": "7b8d4be7-58d3-4f27-a7b0-4a3739a14176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Port 22\n",
            "#GatewayPorts no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVFfrtTwIwJX",
        "outputId": "fec76f0e-1ad0-4ec0-b9db-fcb312654ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:YNbR6JQjiE09CHOTdVSB4/xScTJCvDcJ7wez9oEb4cE root@ade2915666c6\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|  o=+*.+=*o.     |\n",
            "|  .o=.+o@.= .    |\n",
            "|      +B.O *     |\n",
            "|     o .= E      |\n",
            "|        S* O     |\n",
            "|        . O o    |\n",
            "|         o = .   |\n",
            "|          . .    |\n",
            "|                 |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing the public key\n",
        "!more /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWvB8FJ4JSIV",
        "outputId": "d32a6b78-e6ef-4834-a33d-8fd41c3c4dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC1vKRXdsBWrG8HEwSeAbwAYQ+RAmgwKl+OfXOScimi2iV6SDMBYOlrrC1jD83W\n",
            "pwBFa0zBDAoNf8VfshT/+w8TLADog245kTNWpm5cd8xXdDvvn53peMSi04QRD1tNYEOGO/0Km0sd98gKcx5RBoP+XOJNlxoRhyv4\n",
            "rcngADT8DRTlVv8zbjHnTHdzhc9Io9XPfAf8LNPFnvCFIqMsbGvjlYk44RTN6H7G2D/vuTmTRrKhh6IwSezAVgvdCmzwtyh2Mxoe\n",
            "HPcW1t0UncpdSzzQ1q01YAYLegSYKBLUprMVd+7WGATpehRTLQI9EGipaTty4dy4vaSw94KFEdYMK/zeSbRKLk/DnERFBFdCr6J1\n",
            "YEABcXQPzR65T9BqiFCmoI16+hETxmb31RcyGTsEvf7HnJIVEogyWQwkWe/Pb2+3+am/FneeX18MZ77II3kSQQCNfgq0InJf88JF\n",
            "tGYQzawW0NPaJb9VJJskVtRF2uJIaZlAVDab3ikot/rtyYeAcoM= root@ade2915666c6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "6kb4ezaaJVjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1L8HEP3JcLq",
        "outputId": "f973c92a-9e25-420d-fa86-762bf9e8e632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh: connect to host localhost port 22: Cannot assign requested address\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "id": "BrxOK9qRJkoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "id": "wiNjMm34J0ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard location to install Hadoop are\n",
        "\n",
        "/usr/local\n",
        "/opt"
      ],
      "metadata": {
        "id": "HF9xSYjbKR7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively"
      ],
      "metadata": {
        "id": "1UtWhMPYKYma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop directory\n",
        "!ls /usr/local/hadoop-3.2.3/etc/hadoop\n",
        "#we can see various configuration files of hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z3tZLPnKjPp",
        "outputId": "d522fb03-ec75-457f-defe-8ee312086481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  httpfs-log4j.properties     mapred-site.xml\n",
            "configuration.xsl\t\t  httpfs-signature.secret     shellprofile.d\n",
            "container-executor.cfg\t\t  httpfs-site.xml\t      ssl-client.xml.example\n",
            "core-site.xml\t\t\t  kms-acls.xml\t\t      ssl-server.xml.example\n",
            "hadoop-env.cmd\t\t\t  kms-env.sh\t\t      user_ec_policies.xml.template\n",
            "hadoop-env.sh\t\t\t  kms-log4j.properties\t      workers\n",
            "hadoop-metrics2.properties\t  kms-site.xml\t\t      yarn-env.cmd\n",
            "hadoop-policy.xml\t\t  log4j.properties\t      yarn-env.sh\n",
            "hadoop-user-functions.sh.example  mapred-env.cmd\t      yarnservice-log4j.properties\n",
            "hdfs-site.xml\t\t\t  mapred-env.sh\t\t      yarn-site.xml\n",
            "httpfs-env.sh\t\t\t  mapred-queues.xml.template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-env.sh file\n",
        "!cat /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6wWi88OKtm-",
        "outputId": "9fbc9b84-cf29-4a23-c2fb-1f2cb31ba73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-openstack,hadoop-azure-datalake,hadoop-aliyun,hadoop-aws,hadoop-azure\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The only required enviroment variable is JAVA_HOME. All the others are optional.\n",
        "\n",
        "To specify the JAVA_HOME variable in hadoop-env.sh we need to uncomment the export line and update it with the actual directory.\n",
        "\n",
        "In this case it should look like this:\n",
        "\n",
        "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"
      ],
      "metadata": {
        "id": "tzHnO0ZJLRMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "UAZx-ewZLSZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Hadoop home variable\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "EK1S-vhDLc8D",
        "outputId": "473daed5-6162-4ba1-b41d-9afcc2051008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0a3b0f983105>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Creating Hadoop home variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HADOOP_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/usr/local/hadoop-3.2.3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop xml files\n",
        "!ls $HADOOP_HOME/etc/hadoop/*.xml"
      ],
      "metadata": {
        "id": "d_0rNrgcNT5v",
        "outputId": "492fc2af-de4c-4b3b-e836-2e7473986e2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/etc/hadoop/*.xml': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCkDsnJxNUyS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}